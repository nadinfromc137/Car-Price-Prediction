{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Car Price Prediction Project\n"
      ],
      "metadata": {
        "id": "UjtUslaIk1qS"
      },
      "id": "UjtUslaIk1qS"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e53d3d6",
      "metadata": {
        "id": "1e53d3d6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# Imports\n",
        "import numpy as np\n",
        "#from helper import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "#import category_encoders as ce\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "#------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset and clean\n",
        "We found there were some missing values we could logically fill, others not.\n",
        "\n",
        "For example if state value is missing (which is true in 10 records), then use the mileage to set state. If mileage is 0 then set state as New.\n",
        "\n",
        "There are 8 records with model missing, so we filled them with the most freuent model.\n",
        "\n",
        "Other than that we fixed some mistakes like there were models 'A5' vs ' A5' (there is a space), so we combined these."
      ],
      "metadata": {
        "id": "5NDyGVlgk_hF"
      },
      "id": "5NDyGVlgk_hF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q2qGb3lnlYF"
      },
      "outputs": [],
      "source": [
        "def loadAndClean(filepath):\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # nan count in each column\n",
        "    nanCount = df.isnull().sum()\n",
        "    #print(df.isnull().sum())\n",
        "    #Model, mileage and state have missing values\n",
        "    \n",
        "    # Replace missing models withthe most freuent one, A3\n",
        "    frequentModel =df['model'].value_counts()[df['model'].value_counts() == df['model'].value_counts().max()].idxmax()\n",
        "    df['model'] = df['model'].fillna(frequentModel)\n",
        "    \n",
        "    #print(df[df['mileage'].isna()])\n",
        "    # From here we know that 5 missing mileages are of new cars, so put mileage = 0 \n",
        "    # otherwise put mileage = ave of mileages\n",
        "    \n",
        "    df.loc[df.state == 'New', 'mileage'] = 0\n",
        "    \n",
        "    # For the 2 remainig Nan in mileage, set mileage as the median (or mean but of the non-zero mileages so median is better)\n",
        "    df['mileage'] = df['mileage'].fillna(df['mileage'].median())\n",
        "    \n",
        "    #For missing states, put as New if mileage == 0, o.w put as used \n",
        "    #print(df[df['state'].isna()])\n",
        "    df.loc[df.mileage == 0, 'state'] = 'New'\n",
        "    df.loc[df.mileage != 0, 'state'] = 'Used'\n",
        "    \n",
        "    #print(df.isnull().sum()) # No more missing\n",
        "    \n",
        "    # Assume that A1. is A1, A5. is A5    \n",
        "    df.loc[df.model == ' A1.', 'model'] = \" A1\"\n",
        "    df.loc[df.model == ' A5.', 'model'] = ' A5'\n",
        "    \n",
        "    # Mpg has value < 0 so eliminate \n",
        "    df.loc[df.mpg <= 0, 'mpg'] = df['mpg'].median()\n",
        "    \n",
        "    return df, nanCount, frequentModel"
      ],
      "id": "7q2qGb3lnlYF"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9af673eb",
      "metadata": {
        "id": "9af673eb"
      },
      "outputs": [],
      "source": [
        "# Load dataset \n",
        "data, nanCount, Cm = loadAndClean('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing of data \n",
        "We want to remove any categorical data.\n",
        "All of our categorical features are nominal, except for engine size which is ordinal.\n",
        "For ordinal data we will set the values our selves, for nominal data we will use Sklearn's label encoder."
      ],
      "metadata": {
        "id": "8DdqsDeilKVc"
      },
      "id": "8DdqsDeilKVc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88da56ac"
      },
      "outputs": [],
      "source": [
        "def loadAndClean(filepath):\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # nan count in each column\n",
        "    nanCount = df.isnull().sum()\n",
        "    #print(df.isnull().sum())\n",
        "    #Model, mileage and state have missing values\n",
        "    \n",
        "    # Replace missing models withthe most freuent one, A3\n",
        "    frequentModel =df['model'].value_counts()[df['model'].value_counts() == df['model'].value_counts().max()].idxmax()\n",
        "    df['model'] = df['model'].fillna(frequentModel)\n",
        "    \n",
        "    #print(df[df['mileage'].isna()])\n",
        "    # From here we know that 5 missing mileages are of new cars, so put mileage = 0 \n",
        "    # otherwise put mileage = ave of mileages\n",
        "    \n",
        "    df.loc[df.state == 'New', 'mileage'] = 0\n",
        "    \n",
        "    # For the 2 remainig Nan in mileage, set mileage as the median (or mean but of the non-zero mileages so median is better)\n",
        "    df['mileage'] = df['mileage'].fillna(df['mileage'].median())\n",
        "    \n",
        "    #For missing states, put as New if mileage == 0, o.w put as used \n",
        "    #print(df[df['state'].isna()])\n",
        "    df.loc[df.mileage == 0, 'state'] = 'New'\n",
        "    df.loc[df.mileage != 0, 'state'] = 'Used'\n",
        "    \n",
        "    #print(df.isnull().sum()) # No more missing\n",
        "    \n",
        "    # Assume that A1. is A1, A5. is A5    \n",
        "    df.loc[df.model == ' A1.', 'model'] = \" A1\"\n",
        "    df.loc[df.model == ' A5.', 'model'] = ' A5'\n",
        "    \n",
        "    # Mpg has value < 0 so eliminate \n",
        "    df.loc[df.mpg <= 0, 'mpg'] = df['mpg'].median()\n",
        "    \n",
        "    return df, nanCount, frequentModel"
      ],
      "id": "88da56ac"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dc34af33",
      "metadata": {
        "id": "dc34af33"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    df = data\n",
        "    # TODO: Check model & year\n",
        "    categ = [\"model\", \"year\", \"transmission\", \"fuelType\", \"tax\", \"state\"]\n",
        "    \n",
        "    sizesEngine = {\"XXSmall\": 1, 'XSmall': 2, 'Small': 3, 'Medium': 4, 'Large': 5, 'XLarge': 6, 'XXLarge': 7}\n",
        "    df['engineSize'] = df['engineSize'].map(sizesEngine)\n",
        "    \n",
        "    # Encode Categorical Columns\n",
        "    le = LabelEncoder()\n",
        "    #df[\"state\"] = le.fit_transform(df[\"state\"])\n",
        "    df[categ] = df[categ].apply(le.fit_transform)\n",
        "\n",
        "    # Remove ownerName , ID & price cols\n",
        "    df = df.drop(columns = [\"ownerName\", \"ID\"], axis=1)\n",
        "\n",
        "    return df\n",
        "# Standarization train, test data\n",
        "def standarize(Y_train, Y_test):\n",
        "\n",
        "    Y_train_stand = Y_train.copy()\n",
        "    Y_test_stand = Y_test.copy()\n",
        "\n",
        "\n",
        "    # fit on training data column\n",
        "    scale = StandardScaler().fit(Y_train_stand)\n",
        "\n",
        "    # transform the training data column\n",
        "    Y_train_stand = scale.transform(Y_train_stand)\n",
        "\n",
        "    # transform the testing data column\n",
        "    Y_test_stand = scale.transform(Y_test_stand)\n",
        "\n",
        "    return Y_train_stand, Y_test_stand\n",
        "\n",
        "# Normalization\n",
        "def normalize(Y_train, Y_test):\n",
        "\n",
        "    norm = MinMaxScaler().fit(Y_train)\n",
        "\n",
        "    # transform training data\n",
        "    Y_train_norm = norm.transform(Y_train)\n",
        "\n",
        "    # transform testing dataabs\n",
        "    Y_test_norm = norm.transform(Y_test)\n",
        "\n",
        "    return Y_train_norm, Y_test_norm\n",
        "\n",
        "\n",
        "\n",
        "# Detect outliers using z-scores\n",
        "def detect_outliers_zscore(data):\n",
        "    outliers = []\n",
        "    thres = 3\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    # print(mean, std)\n",
        "    for i in data:\n",
        "        z_score = (i-mean)/std\n",
        "        if (np.abs(z_score) > thres):\n",
        "            outliers.append(i)\n",
        "    return outliers\n",
        "\n",
        "# Detect outliers using inter-quartile range\n",
        "def detect_outliers_iqr(temp):\n",
        "    outliers = []\n",
        "    temp = sorted(temp)\n",
        "    q1 = np.percentile(temp, 25)\n",
        "    q3 = np.percentile(temp, 75)\n",
        "    # print(q1, q3)\n",
        "    IQR = q3-q1\n",
        "    lwr_bound = q1-(1.5*IQR)\n",
        "    upr_bound = q3+(1.5*IQR)\n",
        "    # print(lwr_bound, upr_bound)\n",
        "    for i in temp: \n",
        "        if (i<lwr_bound or i>upr_bound):\n",
        "            outliers.append(i)\n",
        "    return outliers\n",
        "\n",
        "# Detect outliers using botplot (1D Array)\n",
        "def detect_outliers_boxplot(data):\n",
        "    plt.boxplot(data, vert=False)\n",
        "    plt.title(\"Detecting outliers using Boxplot\")\n",
        "    plt.xlabel('Sample')\n",
        "    plt.show()\n",
        "\n",
        "# Handling outliers using Median imputation\n",
        "def handle_outliers_medium_imputation(data, outliers):\n",
        "    median = np.median(data)# Replace with median\n",
        "    clean = []\n",
        "    for i in outliers:\n",
        "        clean = np.where(data==i, median, data)\n",
        "    \n",
        "    #print('median = ',median)\n",
        "    return clean\n",
        "\n",
        "\n",
        "# Handling outliers using trimming\n",
        "def handle_outliers_trimming(data, outliers):\n",
        "    copy = data.copy()\n",
        "    clean = []\n",
        "    for i in outliers:\n",
        "        clean = np.delete(copy, np.where(data==i))\n",
        "    return clean\n",
        "\n",
        "def detect_handle_outliers_trimming(sample):\n",
        "    copy = sample.copy()\n",
        "\n",
        "    # IQR\n",
        "    Q1 = np.percentile(copy, 25,\n",
        "                    interpolation = 'midpoint')\n",
        "    \n",
        "    Q3 = np.percentile(copy, 75,\n",
        "                    interpolation = 'midpoint')\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    print(\"Old Shape: \", copy.shape)\n",
        "\n",
        "    # Upper bound\n",
        "    upper = np.where(copy >= (Q3+1.5*IQR))\n",
        "    # Lower bound\n",
        "    lower = np.where(copy <= (Q1-1.5*IQR))\n",
        "\n",
        "    copy.drop(upper[0], inplace = True)\n",
        "    copy.drop(lower[0], inplace = True)\n",
        "\n",
        "    return copy\n",
        "\n",
        "\n",
        "# Recursive Feature Selection with RandomForests\n",
        "# def rfe_selection(X,y,n_features):\n",
        "# \t\"\"\"\n",
        "# \tPerforms the Recursive Feature Elimination method and selects the top ranking features\n",
        "\n",
        "# \tKeyword arguments:\n",
        "# \tX -- The feature vectors\n",
        "# \ty -- The target vector\n",
        "# \tn_features -- n best ranked features\n",
        "# \t\"\"\"\n",
        "\n",
        "\n",
        "# \tclf=RandomForestClassifierWithCoef(n_estimators=10,n_jobs=-1)\n",
        "# \tfs= RFE(clf, n_features, step=1)\n",
        "# \tfs= fs.fit(X,y)\n",
        "# \tranks=fs.ranking_\n",
        "\n",
        "# \tfeature_indexes=[]\n",
        "# \tfor i in xrange(len(ranks)):\n",
        "# \t\tif ranks[i]==1:\n",
        "# \t\t\tfeature_indexes+=[i]\n",
        "\n",
        "# \treturn X[:,feature_indexes[0:n_features]],feature_indexes[0:n_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess, remove outliers, and fit to different models to get best accuracy\n",
        "We observe the data's correlation between price and the features after we preprocessed the data. There are somefeatures that are more significant than other, e.g the important ones are model, year, mpg (negatively correlated).\n",
        "\n",
        "## Modeling\n",
        "We attempted 5 different models to predict he car price.\n",
        "As we fed the data into each model, we tried to pass K features each time, from 1 to 9 features, to see their effect on model accuracy. \n",
        "The best accuracy we got was with Random Forest Classifier, and Bayes classifier, both with 9 features."
      ],
      "metadata": {
        "id": "9ZqSkbaBlzbW"
      },
      "id": "9ZqSkbaBlzbW"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f35b7c16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f35b7c16",
        "outputId": "539516e5-0e4a-465a-df65-89d8105af6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7438, 10)\n",
            "                 model      year     price  transmission   mileage  fuelType  \\\n",
            "model         1.000000  0.146919  0.459745      0.045738 -0.103247 -0.213408   \n",
            "year          0.146919  1.000000  0.741618      0.035126 -0.785996  0.172324   \n",
            "price         0.459745  0.741618  1.000000      0.056138 -0.651557  0.028586   \n",
            "transmission  0.045738  0.035126  0.056138      1.000000 -0.032799  0.083310   \n",
            "mileage      -0.103247 -0.785996 -0.651557     -0.032799  1.000000 -0.279156   \n",
            "fuelType     -0.213408  0.172324  0.028586      0.083310 -0.279156  1.000000   \n",
            "tax           0.439585  0.130854  0.368205      0.042936 -0.176372  0.062920   \n",
            "mpg          -0.404112 -0.414537 -0.609795     -0.049258  0.426112 -0.345812   \n",
            "engineSize    0.376159 -0.013678  0.365456     -0.020940  0.123172 -0.598320   \n",
            "state        -0.069070 -0.222156 -0.227344      0.049052  0.225497 -0.014391   \n",
            "\n",
            "                   tax       mpg  engineSize     state  \n",
            "model         0.439585 -0.404112    0.376159 -0.069070  \n",
            "year          0.130854 -0.414537   -0.013678 -0.222156  \n",
            "price         0.368205 -0.609795    0.365456 -0.227344  \n",
            "transmission  0.042936 -0.049258   -0.020940  0.049052  \n",
            "mileage      -0.176372  0.426112    0.123172  0.225497  \n",
            "fuelType      0.062920 -0.345812   -0.598320 -0.014391  \n",
            "tax           1.000000 -0.606867    0.261067 -0.061312  \n",
            "mpg          -0.606867  1.000000   -0.181549  0.156970  \n",
            "engineSize    0.261067 -0.181549    1.000000 -0.057660  \n",
            "state        -0.061312  0.156970   -0.057660  1.000000  \n",
            "(7438, 9)\n",
            "polynomial_features scores:  [0.5651551340103004, 0.6685501026151847, 0.726806785002167, 0.7269050175152868, 0.8007750688339195, 0.8008559380092712, 0.8010715558265087, 0.8029247322774088]\n",
            "SVR(linear) scores:  [0.0004001859102129357, 0.002699502441695012, 0.017906139044488123, 0.020677223740412233, 0.03060177448367083, 0.032691032166711875, 0.033122487426862746, 0.03470761865908256]\n",
            "Linear Regression scores:  [0.5651551340103004, 0.6685501026151847, 0.726806785002167, 0.7269050175152868, 0.8007750688339195, 0.8008559380092712, 0.8010715558265087, 0.8029247322774088]\n",
            "DecisionTreeRegressor scores:  [0.3430521203665827, 0.6825443407545813, 0.8602963313374946, 0.8582215878198799, 0.8988033754358995, 0.9000925757628935, 0.898508661321803, 0.9014850186593634]\n",
            "RandomForestRegressor scores:  [0.4876313411929075, 0.7833103203618637, 0.9110071608204509, 0.9127251301452741, 0.9377572432162051, 0.937830254524834, 0.9403408269086162, 0.940874225104864]\n",
            "BayesClassifier scores:  [0.5651556520498666, 0.6685494427843792, 0.7268069073983912, 0.7269059925498483, 0.8007760270609138, 0.8008572013016628, 0.80107291783611, 0.8029314606017888]\n",
            "Highest score:  0.940874225104864\n"
          ]
        }
      ],
      "source": [
        "# Integer encoding & Irrelevant columns removal\n",
        "data = preprocess(data)\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "# Summary\n",
        "print(data.shape)\n",
        "#print(data.describe())\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "# Outliers Handling - Medium imputation\n",
        "cols = ['mpg']\n",
        "for feature in cols:\n",
        "    # Detect and handle outliers\n",
        "    outliers = detect_outliers_iqr(data[feature])\n",
        "    # print('Feature '+feature+'outliers: ', outliers)\n",
        "    #print('outliers before handling', outliers)\n",
        "    sample = handle_outliers_medium_imputation(data[feature], outliers)\n",
        "    outliers = detect_outliers_iqr(sample)\n",
        "    #print('outliers after handling: ', outliers)\n",
        "    \n",
        "#print(outliers)\n",
        "#print(data.shape)\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "# Outliers handling - Trimming\n",
        "# from scipy import stats\n",
        "# data = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]\n",
        "\n",
        "# TODO: Try trimming certain feilds\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Correlation\n",
        "corr = data.corr()\n",
        "\n",
        "print(corr)\n",
        "\n",
        "\n",
        "\n",
        "# Correlation output:\n",
        "#   Very low: engineSize, transmission, fuelType\n",
        "#   Average: model, tax \n",
        "#   High: year \n",
        "#   NEGATIVE Average: mileage, mpg, state\n",
        "\n",
        "# output after feature extraction\n",
        "output = data['price']\n",
        "\n",
        "# Remove price column\n",
        "data = data.drop(columns = ['price'], axis=1)\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "print(data.shape)\n",
        "\n",
        "# Loop on k features\n",
        "K = range(2, 10)\n",
        "pf_scores = []\n",
        "svr_rbf_scores = []\n",
        "svr_linear_scores = []\n",
        "svr_poly_scores = []\n",
        "lr_scores = []\n",
        "dt_scores = []\n",
        "rf_scores = []\n",
        "bayes_scores=[]\n",
        "\n",
        "\n",
        "poly_features_switch = True\n",
        "svr_rbf_switch = False\n",
        "svr_linear_switch = True\n",
        "svr_poly_switch = False\n",
        "lr_switch = True\n",
        "dt_switch = True\n",
        "rf_switch = True\n",
        "bayes_switch = True\n",
        "#print(data.head())\n",
        "\n",
        "for k in K:\n",
        "\n",
        "    # Convert to numpy array\n",
        "    X = data.to_numpy()\n",
        "\n",
        "    # Labels (prices)\n",
        "    Y = output.to_numpy()\n",
        "\n",
        "    #------------------------------------------------------------------\n",
        "    # # Feature Selection\n",
        "    #f_statistic, p_values = f_regression(X,Y)\n",
        "\n",
        "    #print(f_statistic) #scores\n",
        "\n",
        "    #print(p_values) #p values\n",
        "    # TODO: Try other filtering, wrapping methods\n",
        "    model = SelectKBest(score_func=f_regression, k=k) \n",
        "    #ridge = Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
        "    #normalize=False, random_state=None, solver='auto', tol=0.001)\n",
        "    #model = ridge.fit(X,Y)\n",
        "    # results = model.fit(X,Y)\n",
        "    \n",
        "    # print (results.scores_)\n",
        "    # print (results.pvalues_)\n",
        "\n",
        "    X = model.fit_transform(X,Y) # New data with top k features\n",
        "\n",
        "    #------------------------------------------------------------------\n",
        "\n",
        "    # Reshape to 2d array -> column\n",
        "    #Y = Y.reshape((len(Y), 1))   \n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=1)\n",
        "\n",
        "    # Standarize\n",
        "    #Y_train_std, Y_test_std = standarize(Y_train, Y_test)\n",
        "\n",
        "    # Shapes\n",
        "    #print('Train', X_train.shape, Y_train.shape)\n",
        "    #print('Test', X_test.shape, Y_test.shape)\n",
        "\n",
        "    #------------------------------------------------------------------\n",
        "    # Reshape\n",
        "    # Y_train_std = Y_train_std.reshape(Y_train_std.shape[0], -1)\n",
        "    # Y_test_std = Y_test_std.reshape(Y_test_std.shape[0], -1)\n",
        "\n",
        "    # Classifiers\n",
        "    # TODO: use scoring='neg_mean_squared_log_error' on crossvalidation\n",
        "    highest_pipe = None\n",
        "    highest_score = 0\n",
        "    if poly_features_switch:\n",
        "        poly_reg = PolynomialFeatures(degree = k)\n",
        "        X_poly = poly_reg.fit_transform(X_train)\n",
        "        poly_features_pipe = Pipeline([('scaler', MinMaxScaler()), ('polynomial_features', LinearRegression() )])\n",
        "\n",
        "        cv_results = cross_validate(poly_features_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # poly_features_pipe.fit(X_train, Y_train)\n",
        "        # score = poly_features_pipe.score(X_test,Y_test)\n",
        "        pf_scores.append(score)\n",
        "\n",
        "    if svr_rbf_switch:\n",
        "        svr_rbf_pipe = Pipeline([('scaler', MinMaxScaler()), ('svc', SVR(kernel='rbf', gamma=0.1))])\n",
        "        cv_results = cross_validate(svr_rbf_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # svr_rbf_pipe.fit(X_train, Y_train)\n",
        "        # score = svr_rbf_pipe.score(X_test,Y_test)\n",
        "        svr_rbf_scores.append(score)\n",
        "\n",
        "    # plt.scatter(X_train, Y_train, color='red', label='Actual observation points')\n",
        "    # plt.plot(X_train, pipe.predict(X_train), label='SVR regressor')\n",
        "    # plt.title('Truth or bluff (SVR Regression)')\n",
        "    # plt.xlabel('Position Level')\n",
        "    # plt.ylabel('Salary')\n",
        "\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    # Train using a linear kernel\n",
        "    if svr_linear_switch:\n",
        "        svr_linear_pipe = Pipeline([('scaler', MinMaxScaler()), ('svc', SVR(kernel='linear'))])\n",
        "        cv_results = cross_validate(svr_linear_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        #svr_linear_pipe.fit(X_train, Y_train)\n",
        "        #score = svr_linear_pipe.score(X_test,Y_test)\n",
        "        svr_linear_scores.append(score)\n",
        "\n",
        "    # Visualize\n",
        "    # plt.scatter(X_train, Y_train, color='red', label='Actual observation points')\n",
        "    # plt.plot(X_train, pipe.predict(X_train), label='SVR regressor')\n",
        "    # plt.title('Truth or bluff (SVR Regression)')\n",
        "    # plt.xlabel('Position Level')\n",
        "    # plt.ylabel('Salary')\n",
        "\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "    if svr_poly_switch:\n",
        "        # Train using a polynomial kernel\n",
        "        svr_poly_pipe = Pipeline([('scaler', MinMaxScaler()), ('svc', SVR(kernel='poly', degree=2))])\n",
        "        cv_results = cross_validate(svr_poly_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # svr_poly_pipe.fit(X_train, Y_train)\n",
        "        # score = svr_poly_pipe.score(X_test,Y_test)\n",
        "        svr_poly_scores.append(score)\n",
        "\n",
        "    # Visualize\n",
        "    # plt.scatter(X_train, Y_train, color='red', label='Actual observation points')\n",
        "    # plt.plot(X_train, pipe.predict(X_train), label='SVR regressor')\n",
        "    # plt.title('Truth or bluff (SVR Regression)')\n",
        "    # plt.xlabel('Position Level')\n",
        "    # plt.ylabel('Price')\n",
        "\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "    if lr_switch:\n",
        "        lr_pipe = Pipeline([('scaler', MinMaxScaler()), ('LR', LinearRegression())])\n",
        "        #pipe.fit(np.array(X_train.reshape(-1, 1)), Y_train.reshape(-1, 1))\n",
        "        cv_results = cross_validate(lr_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # pipe.fit(X_train, Y_train)\n",
        "        # #y_predict = pipe.predict(X_test.reshape(-1, 1))\n",
        "        # score = pipe.score(X_test, Y_test)\n",
        "        lr_scores.append(score)\n",
        "\n",
        "    # Visualize\n",
        "    # plt.scatter(X_train, Y_train, color='red', label='Actual observation points')\n",
        "    # plt.plot(X_train, pipe.predict(X_train), label='LinearRegression')\n",
        "    # plt.title('Truth or bluff (LinearRegression)')\n",
        "    # plt.xlabel('Position Level')\n",
        "    # plt.ylabel('Price')\n",
        "\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    if dt_switch:\n",
        "        dt_pipe = Pipeline([('scaler', MinMaxScaler()), ('LR',  DecisionTreeRegressor(random_state = 0))])\n",
        "        cv_results = cross_validate(dt_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # dt_pipe.fit(X_train, Y_train)\n",
        "        # score = dt_pipe.score(X_test, Y_test)\n",
        "        dt_scores.append(score)\n",
        "\n",
        "\n",
        "    # # Visualize\n",
        "    # plt.scatter(X_train, Y_train, color='red', label='Actual observation points')\n",
        "    # plt.plot(X_train, pipe.predict(X_train), label='DecisionTreeRegressor')\n",
        "    # plt.title('Truth or bluff (DecisionTreeRegressor)')\n",
        "    # plt.xlabel('Position Level')\n",
        "    # plt.ylabel('Price')\n",
        "\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "    if rf_switch:\n",
        "        rf_pipe = Pipeline([('scaler', MinMaxScaler()), ('RF',  RandomForestRegressor(n_estimators=300,random_state = 0))])\n",
        "        cv_results = cross_validate(rf_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # rf_pipe.fit(X_train, Y_train)\n",
        "        # score = rf_pipe.score(X_test, Y_test)\n",
        "        if score > highest_score:\n",
        "            highest_score = score\n",
        "            highest_pipe = rf_pipe\n",
        "        rf_scores.append(score)\n",
        "    \n",
        "    if bayes_switch:\n",
        "        bayes_pipe = Pipeline(steps=[('scaler', MinMaxScaler()), ('Bayes', BayesianRidge())])\n",
        "        cv_results = cross_validate(bayes_pipe, X_train, Y_train, cv=5) # \n",
        "        scores = cv_results['test_score']\n",
        "        score = np.average(scores)\n",
        "        # bayes_pipe.fit(X_train, Y_train)\n",
        "        # score = bayes_pipe.score(X_test, Y_test)\n",
        "        bayes_scores.append(score)\n",
        "\n",
        "print('polynomial_features scores: ',pf_scores) # 90\n",
        "#print('SVR(rbf) scores: ',svr_rbf_scores)\n",
        "print('SVR(linear) scores: ',svr_linear_scores)\n",
        "#print('SVR(poly) scores: ',svr_poly_scores)\n",
        "print('Linear Regression scores: ', lr_scores) #\n",
        "print('DecisionTreeRegressor scores: ',dt_scores) # 90\n",
        "print('RandomForestRegressor scores: ',rf_scores) # 90\n",
        "print('BayesClassifier scores: ',bayes_scores)\n",
        "\n",
        "print('Highest score: ', max(rf_scores))\n",
        "\n",
        "# Show accruacies on graphs (K vs classifier scores)\n",
        "# Uncomment and handle in python the following lines to show graph\n",
        "# %matplotlib inline \n",
        "\n",
        "# plt.plot(K, rf_scores)\n",
        "# plt.xlabel('rf_scores')\n",
        "# plt.ylabel('Testing Accuracy')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using model with the test set"
      ],
      "metadata": {
        "id": "mEdMzHzsnKRj"
      },
      "id": "mEdMzHzsnKRj"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e236047d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e236047d",
        "outputId": "19ddda63-80a7-4342-8c9d-170a6e14b778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ],
      "source": [
        "# Test set\n",
        "test_data, nanCount, Cm = loadAndClean('test.csv')\n",
        "\n",
        "Ids = test_data['ID']\n",
        "\n",
        "test_data = preprocess(test_data)\n",
        "\n",
        "model = SelectKBest(score_func=f_regression, k=9) \n",
        "\n",
        "X = model.fit_transform(X,Y) # New data with top k features\n",
        "\n",
        "pipe = Pipeline([('scaler', MinMaxScaler()), ('RF',  RandomForestRegressor(n_estimators=100,random_state = 0))])\n",
        "\n",
        "pipe.fit(X,Y)\n",
        "\n",
        "concat = [Ids, pipe.predict(test_data)]\n",
        "\n",
        "\n",
        "with open('Submission.txt','w') as f:\n",
        "    for x in zip(*concat):\n",
        "        f.write(\"{0},{1}\\n\".format(*x))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "carPricePrediction.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}